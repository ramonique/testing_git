  Look at linguistic definitions
  Look at our definition
  Describe these examples in the light of those two things

Structural and lexical ambiguity.
We don't consider semantic ambiguity or \emph{vagueness}, such as
determining an absolute size, e.g. \emph{big mouse} is smaller than a
\emph{small house}.
The scope of this work is finding lexical and structural ambiguity;
cases where a sentence corresponds to multiple parse trees when parsed
with a GF grammar, but all the parses can be enumerated and the
correct parse for the given context is among them.

Lexical ambiguity stems from 
----
COPYPASTE FROM FRED'S CG BOOK

DeRose (1988) analyzed the 1 million word Brown corpus and concluded that the rate of categorial ambiguity is around 11.0% for word form types, and over 40% for word form tokens. This figure covers paradigm-external part of speech ambiguities only.
In Swedish, morphological ambiguities are more pervasive than in English. More than 60% of the word-form tokens are at least two-ways ambiguous, as determined on the basis of the 1 million word corpus Press-65 (Allén 1971, Berg 1978). Herz – Rimon (1990) report that the word-form ambiguity rate of Hebrew is nearly 60%.


From Hirst (1987: 137B150) we borrow the class of analytic ambiguities. These occur when there is more than one possible analysis of a subpart, either word or longer constituent, of a sentence. There are several typical English instances. A preposition may be a verb particle or a true preposition starting a prepositional phrase as in example (d) below. Present participle readings and noun readings may be ambiguous (e). The end of a noun group may be unclear in a sequence of nouns (f). Reduced relative clauses and finite verb structures look alike (g). Two syntactic labels might be equally possible (h). Most of these ambiguities are local but some have a global flavour, especially (f) that invokes coordination and gapping.
(d) A good pharmacist dispenses with accuracy.
(e) We discussed running.
(f) Bill gave the dog water and Sue the cat food.
(g) The horse raced past the barn fell.
(h) They are flying planes.
True global ambiguities range over most of the sentence, as in classical laboratory sentences like:
(i) Time flies like an arrow.
In actual usage such instances seem to be rare, perhaps save for intentional puns. A fairly global type of ambiguity is that connected with gap finding and filling, e.g.:
￼￼￼￼￼￼￼￼￼￼(j) Those are the boys that the police debated / about fighting /.
￼
where the slash ”/” indicates a potential gap site (cf. Ford – Bresnan – Kaplan 1982).
In Constraint Grammar, ambiguities are conceived of as strict surface phenomena. A
phrase such as the shooting of the hunters is not surface ambiguous, it consists of a plain nominal head followed by a plain nominal postmodifier. The possible ambiguity must be recognized at some ”deeper” level of syntax or semantics. However, it is not clear how the borderline between surface morphosyntax and such a deeper level of analysis should be conceived.
A particularly recalcitrant type of ambiguity arises in connection with coordination, which, in turn, often co-occurs with various types of ellipses. Do syntagms like old men and women or fuel pump and filter have two surface syntactic analyses, thus being ambiguous and posing a challenge for Constraint Grammar parsing, or do they have only one syntactic, but possibly two semantic analyses? So much basic descriptive data seem to be lacking that this issue and similar ones must be left undecided for the time being. The individual grammar writer will have to make his/her own decisions.

END COPYPASTE
----


ABCD:
  Has three variations of a simple grammar with terms A, B, C, D and S. 
  GrammarA has 1 ambiguity. The sentence a can be either 
          (AtoS aA)
        | (BtoS aB)


  GrammarAS has 1 ambiguity.
  Two consecutive a's can be parsed either as 
    s1 (a a) b
    s1 a (s1 a b).
  
  GrammarABCD has 1 ambiguity.

all:
  Combines the ambiguities of YouWalk (1), Duck (3) and Telescope (3).
  The result should be a sum of the ambiguities

  Duck adds ditransitive verbs to the mix, which adds the ambiguities
  to 7, due to the ambiguities in Telescope: an ambiguous NP can be
  either in a subject, object or indirect object position.

  From a practical point of view, 6 or 7 are both fine numbers.


barn_ar_vilt:
  A complement is ambiguous only if the subject is of neutrum gender.
  NOUN is wild_A =
     NOUN<utrum> är vild -- NOUN<neutrum> är vilt

  NOUN is game_N =
     NOUN<utrum> är vilt -- NOUN<neutrum> är vilt


complement-fixing_mouse_antibody:
  TODO
  Apposition and modification.

delicious_fish:
  This is ambiguous only in the NP level ("fish" can be either Sg or Pl), 
  but not on the sentence level: 
  "the fish is delicious" or "the fish are delicious".

made_her_duck:
  Ambiguity is caused by three factors:
    a) verb "make" can be transitive (V2), ditransitive (V3) or
       take a verbal complement (V2V)
    b) "duck" can be either a verb or a noun, thus it can fit as a complement 
       of any of those verb types
    c) "her" can be direct object, indirect object or possessive, 
       so "made her duck" can be 
         * NP (Poss her) (N duck)
         * (NP her) (V duck)
         * (NP her) (N duck)

  The ambiguities are three:
    a) she made her <NOUN>
        Ambiguous between make_V2 & her_Poss and make_V3 & her_Obj

    b) she made <NOUN> duck
        Ambiguous between make_V2v & duck_V and make_V2 & duck_N

    c) she made her duck
        Ambiguous between
           make_V2V, she_Pron, duck_V
           make_V2 , she_Poss, duck_N
           make_V3 , she_Pron, duck_N

many_tigers_panthers_fish:
  Two varieties of lexical ambiguity: Fish is identical in Sg and Pl,
  and Cat is identical with Tiger in Sg, and with Dog in Pl.

  Each of the four lexical items; Tiger, Cat, Dog and Fish should be reported.
  Nothing with the-functions in this grammar.

obj_or_adv:
  This grammar is ambiguous in Finnish but not in English.
  Problem: part_Prep doesn't have a string linearisation in Finnish,
  it just affects the case of the dependent noun.
  from7behind_Prep has a string and it affects the case of the dependent noun.
  Result: these trees are ambiguous
     a) BasicPP from7behind_Prep (the_NounPhrase_1 0)
     b) BasicPP part_Prep (PossNP (the_NounPhrase_1 0) taka_N)
    in context : PrepVP ( VtoVP ( the_Verb_1 0 ) ) ['*'] ) 


  Variation in ObjAdvWithTransVerbs:
    Adding transitive verbs adds another ambiguity: 
    the string "NOUN<gen> takaa" can be parsed as a PP, 
    or it can be parsed as a NounPhrase in an object case.

orange_and_apple:
 TODO

sharedgrammar:
  This is the base for the grammars. It has categories 
  SimpleNoun; NounPhrase; Verb; Verb2; Verb3; Verb2Verb; VerbPhrase; PP ;  Preposition ; Pronoun ; S .
  It doesn't have any words in it, so it doesn't have ambiguities.

tiger_panther_fish:
  TODO

understand:
  A string can be parsed as a VP either

    a) V2 + noun
    b) V which is a particle verb 

  The offending items are understand_V and the combination get_V2 reason_N.
  This is a single ambiguity.

  In case of other particle verbs, they should be reported as a different
  ambiguity. TODO an example that has more particle verbs in it.

with_telescope:
  Basic PP attachment problem.

you_walk:
  Lexical ambiguity: you can be singular or plural.
